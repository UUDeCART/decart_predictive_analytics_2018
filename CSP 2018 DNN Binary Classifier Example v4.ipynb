{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Toe In The Tensorflow DNN Water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is supplementary material for the talk _Deep Neural Networks for\n",
    "Scalable Prediction_ given at the ASA's __Conference on Statistical Practice__, Portland OR, February 2018.\n",
    "\n",
    "It provides a runnable example of a _very_ simple multilayer perceptron binary classifier deep neural network, with training and evaluation done using the Tensorflow library.  The purpose is to introduce to statistics practitioners some basic deep neural network concepts.  Also included is an \"ordinary\" binary logistic regression model that produces results for comparison purposes.\n",
    "\n",
    "The example provided here doesn't necessarily represent the \"best\" way of training and evaluating the network described, nor do the methods used illustrate the best ways of using the capabilities of the packages employed.  In fact, this example doesn't work exceedingly well for the real world data employed.  Try to improve it by changing the number of layers or by making other modifications.\n",
    "\n",
    "To run what's here you'll need to be using a Python 3.x kernel, and also to install a couple of Python packages.  You'll need to download some publicly available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty or MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the terms of the GNU General Public License for more details. The GNU General Public License can be found at <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is from a Portuguese bank.  It was used for direct marketing as described by Moro, Cortez & Rita (2014) _A data driven approach to predict the success of bank telemarketing_. __Decision Support Systems__, 62, 22-31.  \n",
    "\n",
    "It is available from the University of California Irvine (UCI) Machine Learning Repository at:  \n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Bank+Marketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Libraries Needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to have the packages below installed before you can import them.  Note that if you are using tensorflow 1.4 and Python 3.6x, when you import tensorflow you may get a compile time warning.  A (possible)reason is that a 1.4 module was compiled under Python 3.5.  It's just a warning, and shouldn't affect what will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# The following allows output from multiple statements to come out in a single output cell.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Deep Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, most Deep Learning applications involve four main elements: data, a model or models, an optimization method, and a cost or loss function.  You provide or specify each of these, and then you train your model to do what it's supposed to do.  Training is typically accomplished using a gradient descent method that attempts to minimize the value of the loss function conditional on the values of weights.  A commonly used loss function is what's called *cross entropy loss*.  \n",
    "\n",
    "Some computational libraries used to apply deep learning methods, like Tensorflow and Torch, use *tensors* to contain the data used in training and testing deep learning algorithms. A tensor a a grid of numbers with a variable number of dimensions, or axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The Data and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `bank.zip` file from the UCI repository. Unzip it in a directory it can be accessed from by this Notebook.  This zip file should have three files in it: `bank.csv`, `bank-full.csv`, and `bank-names.txt`.  The text file provides variable definitions and some background information.  \n",
    "\n",
    "The `bank-full` file is a semicolon-delimited file with 45,211 records having 17 columns or variables. It has a header record containing variable names.  The `bank` file has in it a randomly selected 10% of the records in the `bank-full` file.  In what follows the full data set is used, but you can used the smaller dataset if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a Pandas DataFrame.  Read `bank.csv` if you want less data\n",
    "bankDat=pd.read_csv('bank-full.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As read in, most of the 17 variables are type \"object\" (i.e., character) and some are integer.  The variable we want to predict is `y`, which is coded \"yes\" or \"no\", indicating whether a customer responded to the most recent marketing campaign.  \n",
    "\n",
    "To keep this example very simple, we're going to use a subset of the variables, those that can be used with the least amount of transformation. Deep Neural networks implemented in Tensorflow like to have continuous, rather than discrete, input data as their inputs are used in mathematical operations like addition and multiplication. Discrete or categorical inputs used are transformed in some way, like by dummy or \"one hot\" coding, binning (\"bucketizing\"), hashing, or embedding. Check out some Tensorflow documentation about encoding categorical features, e.g. https://www.tensorflow.org/versions/master/get_started/feature_columns\n",
    "\n",
    "We're going to \"fudge\" a little here by treating some binary input variables as truncated continuous variables by coding them as 0 or 1.\n",
    "\n",
    "Note that the percentage of \"yes's\" in the `y` data, the percent responding to the campaign, is $\\approx$ 12%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Variables as Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use just \"features\" (inputs) that are we'll treat as continuous measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "varsToUse=['age','balance','housing','loan','campaign','previous','y']\n",
    "\n",
    "campCusts=bankDat[varsToUse]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`campCusts` is a DataFrame with the vars we're going to use. `y` will be the binary dependent variable, or *output*.  The other variables will be our inputs, or *features*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`housing`, `loan`, and `y` are character types with values \"yes\" or \"no\". We'll convert these to numeric 1|0 variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a stone simple function for creating new 0/1 vars.\n",
    "# Another way to do this would be to use the numpy .where() method.\n",
    "\n",
    "def zero1(z):\n",
    "    if z=='no':\n",
    "        return(0)\n",
    "    return(1)\n",
    "\n",
    "cVars=['housing','loan','y']   # vars to be made 0|1\n",
    "\n",
    "new01Vars=campCusts[cVars].applymap(zero1)   # create a new df that's 0|1\n",
    "\n",
    "contVars=campCusts.loc[:,~(campCusts.columns.isin(cVars))]\n",
    "\n",
    "campCusts2=contVars.join(new01Vars)  # join on the DataFrame indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Input Data Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "campCustX=campCusts2.iloc[:,0:6].values    #Here are the input, or predictor, variables\n",
    "\n",
    "convY=campCusts2.y.values          # Our 0|1 output, or dependent, variable. Approx. 12% 1's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Split into Training and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a numpy function to make \"coin flips\" to get an 80/20 split.  We'll use the 80% for training, and the 20% for testing, for evaluating the NN's predictive accuracy using held out data.  We'll use a method from `sklearn` to do the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, yTrain, yTest = train_test_split(campCustX, convY,\n",
    "                                   test_size=0.20, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling (Standardizing) Predictor Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a common machine learning practice to rescale continuous input variables so that they have the same mean and variance.  We'll do that here using `sklearn` methods.  You can skip this rescaling to see if it makes a difference in the results, below, if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=Xtrain.astype(float)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(Xtrain)\n",
    "\n",
    "# XTrain's columns should be mean = 0, std = 1\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "\n",
    "# Xtest is rescaled based on Xtrain's means and std devs\n",
    "Xtest = scaler.transform(Xtest.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Ordinary\" Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of comparison to results that follow, let's use some `scikit learn` methods to estimate a logistic regression using the training data.  Then, check the model's predictive accuracy and AUC using the training data and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "logReg=linear_model.LogisticRegression()\n",
    "logMod1=logReg.fit(Xtrain,yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data accuracy 0.882\n",
      "training data auc 0.688\n",
      "test data accuracy 0.881\n",
      "test data auc 0.689\n"
     ]
    }
   ],
   "source": [
    "print('training data accuracy {:5.3f}'.format(logMod1.score(Xtrain,yTrain)))\n",
    "trainPredProbs=logMod1.predict_proba(Xtrain)\n",
    "regTrainAUC=roc_auc_score(yTrain,trainPredProbs[:,1])\n",
    "print('training data auc {:5.3f}'.format(regTrainAUC))\n",
    "\n",
    "print('test data accuracy {:5.3f}'.format(logMod1.score(Xtest,yTest)))\n",
    "trainPredProbs=logMod1.predict_proba(Xtest)\n",
    "regTestAUC=roc_auc_score(yTest,trainPredProbs[:,1])\n",
    "print('test data auc {:5.3f}'.format(regTestAUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Data Feeder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're doing to do \"mini-batch\" training. We're going to use randomly selected with replacement samples (\"mini-batches\") from our training data in each iteration of our training algorithm.  Once we're done we'll use the observations we've set aside ('train' = False) to assess our network's predictive accuracy using data it didn't learn it's parameters, i.e. weights, from.  This function will grab a random subset of the training data on each iteration of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(epoch, ncases, b_ndx, b_size):\n",
    "    # epoch is the alg iteration, b_ndx is the batch no.\n",
    "    # b_size is batch size\n",
    "    \n",
    "    ndxs= np.random.randint(ncases,size=b_size)\n",
    "    X_bat=Xtrain[ndxs]\n",
    "    y_bat=yTrain[ndxs]\n",
    "    return X_bat, y_bat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Hidden Layer NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A two hidden layer fully connected NN with four nodes per hidden layer is specified here.  Feel free to fiddle around with it, of course.\n",
    "\n",
    "There are more elegant and more efficient ways of doing what follows using methods in the `tensorflow` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters  - change 'em if you wish\n",
    "\n",
    "learn_rate = 0.1\n",
    "b_size = 100                 # batch size\n",
    "n_epochs = 100               # no. of epochs\n",
    "ncases=Xtrain.shape[0]       #no. of records\n",
    "n_bats = int(np.ceil(ncases/b_size))  # no. of batches\n",
    "\n",
    "# Inputs, nodes in hidden layers, classes in the output\n",
    "\n",
    "n_hid_1 = 4 # 1st layer number of neurons\n",
    "n_hid_2 = 4 # 2nd layer number of neurons\n",
    "num_inp = 6 # selected campaign predictors\n",
    "num_class = 2 # y values, 0 or 1\n",
    "\n",
    "\n",
    "# tf placeholders for input data\n",
    "\n",
    "X = tf.placeholder(\"float32\", shape=(None, num_inp),name=\"X\")\n",
    "y = tf.placeholder(\"int32\", shape=(None),name=\"y\")\n",
    "\n",
    "# Layer weights & biases\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_inp, n_hid_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hid_1, n_hid_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hid_2, num_class]))\n",
    "     }\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hid_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hid_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_class]))\n",
    "     }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of network layers.  A \"slicker\" way to would be to use \"named scopes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(x):\n",
    "    # A two hidden fully connected layers each with 4 neurons, relu activation fcns\n",
    "    layer_1 = tf.nn.relu(tf.matmul(X, weights['h1']))\n",
    "    # Hidden fully connected layer with 4 neurons\n",
    "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights['h2']))\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out'])\n",
    "    return out_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specification of of the loss function, optimization method, accuracy calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = neural_net(X)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learn_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "correct_pred=tf.nn.in_top_k(logits,y,1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intialization of All Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Algorithm in Tensorflow Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a `Tensorflow` session that runs the specified number of epochs with min-batches of the specified size.  When it's done iterating, it prints out classification accuracies and AUC estimates based on all of the training data, and all of the test data.\n",
    "\n",
    "This part might take a bit to run, and especially so if you are running a non-gpu version of Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, 20, 40, 60, 80, done!\n",
      "\n",
      "\n",
      "training data accuracy 0.887\n",
      "training data auc 0.699\n",
      "test data accuracy 0.884\n",
      "test data auc 0.699\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print('epoch ',end='')\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for b_ndx in range(n_bats):\n",
    "            X_batch, y_batch=get_batch(epoch,ncases, b_ndx, b_size)\n",
    "            sess.run(train_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 20 == 0:\n",
    "            print(epoch, end=', ')\n",
    "    \n",
    "    print('done!\\n')\n",
    "    accTrain=accuracy.eval(feed_dict={X: Xtrain, y: yTrain})\n",
    "    print('\\ntraining data accuracy {:5.3f}'.format(accTrain))\n",
    "    trainProbs=tf.nn.softmax(logits).eval(feed_dict={X: Xtrain, y: yTrain})\n",
    "    print('training data auc {:5.3f}'.format(roc_auc_score(yTrain,trainProbs[:,1])))\n",
    "    \n",
    "    accTest=accuracy.eval(feed_dict={X: Xtest, y: yTest})\n",
    "    print('test data accuracy {:5.3f}'.format(accTest))\n",
    "    testProbs=tf.nn.softmax(logits).eval(feed_dict={X: Xtrain, y: yTrain})\n",
    "    print('test data auc {:5.3f}'.format(roc_auc_score(yTrain,testProbs[:,1])))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
