{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias, Variance, and the Bias/Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Note**: there is an **EXERCISE** here, way down below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following allows multiple outputs in a single output cell.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned earlier, in the context of machine learning and when \"learners\" are being discussed, _bias_ and _variance_ \n",
    "\n",
    "**BIAS** = error in approximating the \"true state of Nature,\" real processes or systems.\n",
    "\n",
    "**VARIANCE** = how much our approximation of $f(x_i),~\\hat{f}(x_i)$, varies when applied to new samples from the same population, samples not used to estimate $\\hat{f}(x_i)$.  \n",
    "\n",
    "In principle, we could minimize _both_ bias and variance with the \"best\" model and lots of data.  It's not that easy, unfortunately.  bias and variance usually vary in different ways as a given model is adjusted in various ways, e.g. by adding or modifying predictor variables.\n",
    "\n",
    "<div style=\"text-align: right\">[James et al., Introduction to Statistical Learning, 2013](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _expected_ MSE for held out a test data observation \"j\" can be decomposed as follows:  \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{E}[y_j-\\hat{f}(x_j)]^2=Var(\\hat{f}(x_j))+Bias[\\hat{f}(x_j)]^2+Var(\\epsilon_j)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All quantities on the RHS of this equation are positive.  \n",
    "\n",
    "It turns out that: \n",
    "\n",
    "* models that have _more_ flexibility, or that are more extensively parameterized, will tend to have relatively _less_ bias, but _higher_ variance;\n",
    "* models that are simpler, that are _less_ flexible, will have _less_ variance, but _more_ bias;\n",
    "* As a model is extended to be more flexible, the _trade-off between bias and variance_ may not be inconstant proportion;\n",
    "* MSE can never be less than $Var(\\epsilon_j)$.\n",
    "\n",
    "Also, it should be expected that simpler models will tend to generalize better, _ceterus paribus_.  A reason is that the more flexible or complicated a model is, the more likely it can be that it is learning the training data per se, and not more general patterns the data are consistent with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Long, but good.)\n",
    "\n",
    "Wolpert's _no free lunch_ theorem indicates that no learning algorithm will do any better on average than any other when generalizing from all possible data-generating mechanisms. That is, no algorithm can be universally better than all others.  An implication is that machine learning algorithms should be developed for specific tasks. \n",
    "\n",
    " \n",
    "<div style=\"text-align: right\">[Goodfellow, Bengio, & Courville, Deep Learning, 2016](https://mitpress.mit.edu/books/deep-learning)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is \"regularization?\" Basically, it's doing things to a model or algorithm to make learning what's needed possible, or to minimize overfitting.  \n",
    "\n",
    "You've already done it.  Recall the regression exercise in which you effects coded the patient category variable.  By doing so you imposed _identification constraints_ in order to be able to estimate regression coefficients for all levels of of this variable.  \n",
    "\n",
    "You _did_ estimate coefficients for all three patient categories, right?  Do you remember how you did this?  \n",
    "\n",
    "Regularization methods usually take the form of some kind of _restriction_.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the implication of the no free lunch theorem, it's not surprising that regularization methods vary greatly.  \n",
    "\n",
    "For example, in conventional logistic regression modeling, the scale factor, or error disturbance, let's call it $\\tau$, and all other coefficients are not simultaneously estimable. So a convention is to assume that $\\tau=1.0$.   As a result, the $\\beta$ coefficient estimates really each estimate $\\frac{\\beta}{\\tau}$'s.  \n",
    "\n",
    "**QUESTION**:  What do you think the above indicates for comparing regression coefficients of different logistic regression models?\n",
    "\n",
    "Another example of regularization is the common econometric practice of mean-centering continuous variables before using them to form interaction terms.  \n",
    "\n",
    "It's not unusual to constrain elements of variance/covariance matrices when estimating multivariate limited dependent variable regression models. \n",
    "\n",
    "When estimating the parameters of _structural equation models_ it can be necessary to constrain coefficients in some way in order for a particular model to be estimable.  When estimating confirmatory factor analysis (CFA) models, done typically using a MLE method, it's common practice to set one factor loading to 1 in order to identify the measurement scale of each factor.  \n",
    "\n",
    "To improve predictive accuracy, _Ridge regression_ constrains the sum of regression coefficients to a constant, which can help when the covariance matrix of predictor variations is close to, or isn't, positive definite. Ridge estimates are _shrinkage estimators_.  The _Lasso_ is a regression method that both reduces covariate collinearity, and does covariate selection, at the same time, also by constraining regression coefficients.  The extension of the Lasso referred to as `Elastic Net` can be used when the number of predictors exceeds the number of cases.\n",
    "\n",
    "**Overfitting** by statistical models is usually avoided by usually manual, careful elimination of selected predictor variables based on a measurable criterion like information loss, decrease in negative log likelihood, or difference in $R^2$. \"Automatic\" variable selection methods like stepwise procedures are not necessarily guaranteed to produce the best while most parsimonious results.\n",
    "\n",
    "So-called \"shrinkage estimators\" may be less likely to exploit idiosyncratic noise present in training data, and thereby be less prone to overfitting.  Estimators.  A shrinkage estimator is an estimator that is adjusted, or \"shrunk,\" using additional information.  Bayesian estimators are shrinkage estimators.\n",
    "\n",
    "Regularization tactics used in ML, AI and \"Deep Learning\" applications are varied and widely used.  \"Conventional\" recursive partitioning algorithms like CART are \"greedy\" and will end up overfitting.  The trees produced by such methods are often trimmed, or \"pruned,\" to improve performance with new data. Some emsemble techniques like RandomForest may be less prone to overfitting.  Regularization methods used when training deep neural networks include \"early stopping,\" which consists of ending training when model performance (e.g. in terms MSE or log likelihood) computed based on test data reaches a minimum.  \"Drop out\" is another method used with networks.  It consists of dropping some randomly selected network nodes or units during each iteration of a algorithm's processing.\n",
    "\n",
    "**QUESTION:**  Can you think of any other examples of regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations, recoding, and Rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often the case that predictor variables are transformed in order to improve modeling performance, and sometimes dependent variables are transformed, as well.   Transformations can take the form of rescaling, applying statistical or mathematical functions (e.g. by taking the log), or other sorts of manipulations.  \n",
    "\n",
    "There are cases in which transformation is unavoidable. An example is when using the TensorFlow<sup>TM</sup> computational library developed at Google and open-sources. This library is widely used for scalable, deep learning applications like various kinds of neural networks.  A tensor is a kind of array, and computations using TensorFlow<sup>TM</sup> are done with matrix algebra.  As a result, input data must be continuous.  Discrete input data is usually transformed in some way, e.g. by dummy coding, hashing, or embedding.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common transformation is rescaling continuous variables so that  they are standardized in some way.  Common versions include converting values on variables to standard normal scores, or rescaling so that the values on variables all have the same range, e.g. 0 to 1.  Sometimes the normal standardization rescaling is used in clustering applicationss, and is called \"sphering\" the data\n",
    "\n",
    "`scikit-learn` has methods for rescaling data. See [scikit-learn preprocessing data](http://scikit-learn.org/stable/modules/preprocessing.html)  \n",
    "\n",
    "Let's apply a couple of these methods to some of the patient satisfaction data. First, we need to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patSatDF=pd.read_csv('DECART-patSat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got it, let's pick out the continuous predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q2    int64\n",
       "q3    int64\n",
       "q4    int64\n",
       "q5    int64\n",
       "q6    int64\n",
       "q7    int64\n",
       "q8    int64\n",
       "q9    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patContPreds=patSatDF[['q2','q3','q4','q5','q6','q7','q8','q9']]\n",
    "patContPreds.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lynd/anaconda36/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "normalizedPreds=preprocessing.scale(patContPreds.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you get a warning about data type and that the data was cast as float? You could have cast the data to float ahead of time, or \"in line,\" like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedPreds=preprocessing.scale(patContPreds.astype(float).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0             1             2             3             4  \\\n",
      "count  1.811000e+03  1.811000e+03  1.811000e+03  1.811000e+03  1.811000e+03   \n",
      "mean  -2.851882e-16 -6.056877e-17 -7.135834e-17  4.514457e-16 -1.352375e-16   \n",
      "std    1.000276e+00  1.000276e+00  1.000276e+00  1.000276e+00  1.000276e+00   \n",
      "min   -1.845736e+00 -1.984986e+00 -1.965345e+00 -1.985241e+00 -2.582398e+00   \n",
      "25%   -7.116166e-01 -4.796218e-01 -4.356405e-01 -4.145676e-01 -9.294077e-01   \n",
      "50%    4.446299e-02  2.730603e-01 -5.321444e-02 -2.189923e-02  3.103350e-01   \n",
      "75%    8.005426e-01  6.494013e-01  7.116376e-01  7.634375e-01  7.235826e-01   \n",
      "max    1.556622e+00  1.402083e+00  1.476490e+00  1.548774e+00  1.136830e+00   \n",
      "\n",
      "                  5             6             7  \n",
      "count  1.811000e+03  1.811000e+03  1.811000e+03  \n",
      "mean   1.381802e-16 -1.120645e-16  2.896021e-16  \n",
      "std    1.000276e+00  1.000276e+00  1.000276e+00  \n",
      "min   -2.829377e+00 -3.222872e+00 -3.062199e+00  \n",
      "25%   -6.381343e-01 -4.161294e-01 -3.497302e-01  \n",
      "50%    2.383626e-01  5.166100e-02  1.023479e-01  \n",
      "75%    6.766111e-01  9.872417e-01  1.006504e+00  \n",
      "max    1.114860e+00  9.872417e-01  1.006504e+00  \n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(normalizedPreds).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll note that as we changed the DataFrame to a np array, we lost the variable names.  We could add them to a new DataFrame of standardized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedPredDF=pd.DataFrame(normalizedPreds)\n",
    "normalizedPredDF.columns=patContPreds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE** Rescale each of the variables in patContPred to have a range of $[0,1]$ using a scaler method from `scikit-learn`.  \n",
    "\n",
    "**EXERCISE** (As time allows) Re-estimate the regression model that predicted patSat using q2, q6, and ptCat, after having rescaled (your choice of method) q2 and q6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A colleague and friend once opined that \"the problem with missing data is that it's not there.\"  The problem with this problem is that it's not always clear what best be done about it.  \n",
    "\n",
    "Three common ways of ignoring what's missing are eliminating cases with any missing data, only using non-missing data for particular analyses, and imputing missing values.\n",
    "\n",
    "In selecting an imputation method it's critically important to take into account the _missing data generating mechanism(s)_ if minimally biased estimates are to be obtained.  A key distinction to be made is whether what's missing is _missing completely at random_ (MCAR), _missing at random_(MAR), or _not missing at random_ (NMAR).\n",
    "\n",
    "Simply put:\n",
    "\n",
    "MCAR is when the probability of a data point being missing is unrelated to what the \"true\" value of it, is.\n",
    "\n",
    "MAR is when the probability of a data point being missing is unrelated to what the true value of it is, _conditional on available nonmissing data_.\n",
    "\n",
    "An example of what might be MAR data is when some survey respondents decline to report their household income, but whether income is reported or not is not related to what their actual income is, conditional on all other data available about them.\n",
    "\n",
    "NMAR is neither of the above.  It's _nonignorable_.  Rejecting a test of MAR supports that conclusion that missingness on a variable is nonignorable, that the data is NMAR.\n",
    "  \n",
    "For example, the probability of survey respondents reporting their income is a function of what their income actually is, regardless of what else is known about them.\n",
    "\n",
    "<div style=\"text-align: left\">See [Wikipedia: Missing Data](https://en.wikipedia.org/wiki/Missing_data)\n",
    "</div>\n",
    "\n",
    "#### What's to be done about what's not there?  \n",
    "\n",
    "It depends on how it got to be missing.  MCAR is the easiest case in that measures of central tendency can be \"plugged in\" as reasonable estimates.\n",
    "\n",
    "When data are other than (just) MCAR, <a href=\"https://en.wikipedia.org/wiki/Imputation_(statistics)#Multiple_imputation\">multiple imputation</a>   might provide reasonable results, at least where the data are MAR.  Generally speaking, this method consists of imputing missing values using the observed data in multiple copies (e.g. 4) of a data set with missings, calculating any estimands of interest using each one, and the summarizing the estimands obtained by, for example, averaging them. \n",
    "\n",
    "As usual, there potential [pitfalls](https://www.bmj.com/content/338/bmj.b2393) when doing using multiple imputation.\n",
    "\n",
    "The `statsmodels` package provides a multiple imputation method called [multiple imputation with chained equations (\"MICE\")[http://www.statsmodels.org/dev/generated/statsmodels.imputation.mice.MICE.html)."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
